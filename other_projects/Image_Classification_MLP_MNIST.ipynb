{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification_MLP_MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krajkumar6/deeplearning/blob/master/Image_Classification_MLP_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GcMbjgq3E6x5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Multi-Layer Perceptron, MNIST**\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we will train an MLP to classify images from the MNIST database hand-written digit database.\n",
        "\n",
        "The process will be broken down into the following steps:\n",
        "\n",
        "1. Load and visualize the data\n",
        "2. Define a neural network\n",
        "3. Train the model\n",
        "4. Evaluate the performance of our trained model on a test dataset!\n",
        "\n",
        "Before we begin, we have to import the necessary libraries for working with data and PyTorch."
      ]
    },
    {
      "metadata": {
        "id": "Gen-7x-05I61",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3S4xsMDCF8OA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Load and Visualize the Data**\n",
        "Downloading may take a few moments, and you should see your progress as the data is loading. You may also choose to change the  batch_size if you want to load more data at a time.\n",
        "\n",
        "This cell will create DataLoaders for each of our datasets."
      ]
    },
    {
      "metadata": {
        "id": "UzKKvbowF2dO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2\n",
        "\n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.MNIST(root='data',train = True, download = True, transform = transform)\n",
        "test_data = datasets.MNIST(root='data',train=False, download = True, transform = transform)\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "#prepare data samplers\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# Prepare data loaders\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size= batch_size,sampler=train_sampler,  num_workers = num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size= batch_size, sampler=valid_sampler,num_workers = num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size= batch_size, num_workers = num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQHPOPLvIASg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize a Batch of Training Data\n",
        "The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data."
      ]
    },
    {
      "metadata": {
        "id": "M4EyjCw3H8L-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "    \n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    # print out the correct label for each image\n",
        "    # .item() gets the value contained in a Tensor\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZtCKaM_IlOY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the Network Architecture\n",
        "The architecture will be responsible for seeing as input a 784-dim Tensor of pixel values for each image, and producing a Tensor of length 10 (our number of classes) that indicates the class scores for an input image. This particular example uses two hidden layers and dropout to avoid overfitting"
      ]
    },
    {
      "metadata": {
        "id": "_7uj2UaMIfXG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    \n",
        "    #number of hidden nodes in each layer is 512\n",
        "    hidden_1 = 512\n",
        "    hidden_2 = 512\n",
        "    \n",
        "    #linear layer (784 ---> hidden_1)\n",
        "    self.fc1 = nn.Linear(28 * 28, hidden_1)\n",
        "    \n",
        "    #linear layer (hidden_1 ---> hidden_2)\n",
        "    self.fc2 = nn.Linear(hidden_1,hidden_2)\n",
        "    \n",
        "    #linear layer (hidden_2 ---> 10)\n",
        "    self.fc3 = nn.Linear(hidden_2, 10)\n",
        "    \n",
        "    #dropout layer\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    \n",
        "    #flatten the image\n",
        "    x = x.view(-1,28*28)\n",
        "    \n",
        "    # add hidden layer, with relu activation function\n",
        "    x = F.relu(self.fc1(x))\n",
        "    \n",
        "    # add dropout layer\n",
        "    x = self.dropout(x)\n",
        "    \n",
        "    # add hidden layer, with relu activation function\n",
        "    x = F.relu(self.fc2(x))\n",
        "    \n",
        "     # add dropout layer\n",
        "    x = self.dropout(x)\n",
        "    \n",
        "    # add output layer\n",
        "    x = self.fc3(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  \n",
        "# initialize the NN\n",
        "model = Net()\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFdrSg-NMTHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Specify Loss Function and Optimizer\n",
        "It's recommended that you use cross-entropy loss for classification. If you look at the documentation (linked above), you can see that PyTorch's cross entropy function applies a softmax funtion to the output layer and then calculates the log loss."
      ]
    },
    {
      "metadata": {
        "id": "2m3GdHzeMOc1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PC8u6uNyNE2D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train the Network\n",
        "The steps for training/learning from a batch of data are described in the comments below:\n",
        "\n",
        "\n",
        "1.  Clear the gradients of all optimized variables\n",
        "2.  Forward pass: compute predicted outputs by passing inputs to the model\n",
        "3. Calculate the loss\n",
        "4. Backward pass: compute gradient of the loss with respect to model parameters\n",
        "5. Perform a single optimization step (parameter update)\n",
        "6. Update average training loss\n",
        "\n",
        "\n",
        "The following loop trains for 50 epochs; take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data."
      ]
    },
    {
      "metadata": {
        "id": "CXShJ3rNNBXU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "model.train() #prep the model for training\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  \n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  #train the model\n",
        "  for data,target in train_loader:\n",
        "    # clear the gradients of all optimized variables\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "#     print(data.size(0))\n",
        "    output = model(data)\n",
        "    \n",
        "#     print(output.shape)    \n",
        "    loss = criterion(output, target)\n",
        "    \n",
        "#   print(loss.item())\n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step() #update parameters\n",
        "    \n",
        "    train_loss += loss.item()*data.size(0)\n",
        "  \n",
        "  \n",
        "  #  Calculating Validation Loss\n",
        "  model.eval() #set model in evaluation mode\n",
        "  for data,target in valid_loader:\n",
        "    output = model(data)\n",
        "    loss = criterion(output,target)\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "  #   print training & validation statistics\n",
        "  train_loss = train_loss/len(train_loader.dataset)\n",
        "  valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "  \n",
        "  print('Epoch: {} \\tTraining Loss: {:.6f} \\t Validation Loss: {:.6f}'.format(epoch+1,train_loss,valid_loss))\n",
        "  \n",
        "  if valid_loss <= valid_loss_min:\n",
        "    print('Validation loss decreased from {:.6f} to {:.6f}. Saving model..'.format(valid_loss_min,valid_loss))\n",
        "    valid_loss_min = valid_loss\n",
        "    torch.save(model.state_dict(),'model.pt')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "khfnwTZpjA0x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Test the Trained Network\n",
        "Finally, we test our best model on previously unseen test data and evaluate it's performance. Testing on unseen data is a good way to check that our model generalizes well. It may also be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its overall loss and accuracy."
      ]
    },
    {
      "metadata": {
        "id": "UxYqiJbkibm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "# print(class_correct)\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval() #put the model in evaluation mode\n",
        "\n",
        "for data, target in test_loader:\n",
        "  output = model(data)\n",
        "  loss = criterion(output, target) # calculate test loss\n",
        "  test_loss += loss.item()*data.size(0)\n",
        "  #   convert output probabilities to predicted class\n",
        "  _,pred = torch.max(output,1)\n",
        "  correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "  \n",
        "  # calculate test accuracy for each object class\n",
        "  for i in range(len(target)):\n",
        "    label = target.data[i]\n",
        "    class_correct[label] += correct[i].item()\n",
        "    class_total[label] += 1\n",
        "    \n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss:{:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "  if class_total[i] > 0:\n",
        "#     print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i],np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    print('Test Accuracy of {0}: {1:2.2f} ({2:5.2f}/{3:5.2f})'.format(str(i), 100 * class_correct[i] / class_total[i],class_correct[i],class_total[i]))\n",
        "  else:\n",
        "    print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total),np.sum(class_correct), np.sum(class_total)))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OKwWCdcQ1xRI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}